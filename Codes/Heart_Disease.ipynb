{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import t\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file, y_name):\n",
    "    df = pd.read_csv(file)\n",
    "    x = df.loc[:, df.columns != y_name]\n",
    "    y = df.loc[:, df.columns == y_name]\n",
    "    return x, y\n",
    "\n",
    "x, y = read_data(\"/Users/abjain/Documents/Industry/ML/ML_practice/Project1/ML_heardisease/data/heart.csv\",'target')\n",
    "# print(x.shape)\n",
    "# print(x.head())\n",
    "# print(y.shape)\n",
    "# y.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(177372)\n",
    "\n",
    "def shuffle(x, y):\n",
    "    idx = np.random.permutation(x.index)\n",
    "    x = x.reindex(idx)\n",
    "    y = y.index(idx)\n",
    "    return x, y\n",
    "\n",
    "def data_split(x, y, frac):\n",
    "    idx = np.random.permutation(x.index)\n",
    "    train_idx = idx[:int(len(idx)*frac)]\n",
    "    test_idx = idx[int(len(idx)*frac):len(idx)]\n",
    "\n",
    "    train_x = x.iloc[train_idx,:]\n",
    "    train_y = y.iloc[train_idx,:]\n",
    "    test_x = x.iloc[test_idx,:]\n",
    "    test_y = y.iloc[test_idx,:]\n",
    "\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "# 80% data in traing while for testing 20% \n",
    "train_x, train_y, test_x, test_y = data_split(x,y,0.8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 13)\n",
      "(242, 1)\n",
      "(61, 13)\n",
      "(61, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you go down in the section of the feature selection we have selected features based on the Lasso regression \n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, \n",
    "                                                test_size=0.2, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 13)\n",
      "(242, 1)\n",
      "(61, 13)\n",
      "(61, 1)\n",
      "(242, 13)\n",
      "(242, 1)\n",
      "(61, 13)\n",
      "(61, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the training and testing dataset with the min-max transformation using sklearn. From now we will use sklearn. Since it is much easier and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "xtrain_scaled = scaler.fit_transform(xtrain)\n",
    "xtest_scaled = scaler.fit_transform(xtest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the K nearest neighbor using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# print(pd.DataFrame(xtrain_scaled).head(2))\n",
    "# ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn KNN Predictions:\n",
      " [0 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# KNeighborsClassifier paramteres\n",
    "# https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "# weights{‘uniform’, ‘distance’}, callable or None, default=’uniform’\n",
    "# Weight function used in prediction. Possible values:\n",
    "# ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n",
    "# ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "# [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n",
    "\n",
    "# algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
    "# Algorithm used to compute the nearest neighbors:\n",
    "# ‘ball_tree’ will use BallTree\n",
    "# ‘kd_tree’ will use KDTree\n",
    "# ‘brute’ will use a brute-force search.\n",
    "# ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n",
    "# Note: fitting on sparse input will override the setting of this parameter, using brute force.\n",
    "\n",
    "# class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors=3, weights='distance', algorithm = 'auto')\n",
    "knn_sklearn.fit(xtrain_scaled, ytrain)\n",
    "\n",
    "# Make predictions\n",
    "predictions_sklearn = knn_sklearn.predict(xtest_scaled)\n",
    "print(\"Scikit-learn KNN Predictions:\\n\", predictions_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccuracyL 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(ytest, predictions_sklearn)\n",
    "print(f\"AccuracyL {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[25  4]\n",
      " [ 5 27]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(ytest, predictions_sklearn)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85        29\n",
      "           1       0.87      0.84      0.86        32\n",
      "\n",
      "    accuracy                           0.85        61\n",
      "   macro avg       0.85      0.85      0.85        61\n",
      "weighted avg       0.85      0.85      0.85        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Classification Report:\\n\", classification_report(ytest, predictions_sklearn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the Hyperparameters for Better Accuracy\n",
    "\n",
    "Grid Search for Hyperparameter Tuning\n",
    "\n",
    "To improve the accuracy or the F1 score, lets tune the parameters using GridSearchCV, which allows to test different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Use GridSearchCV to search for the best parameters\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(xtrain_scaled, ytrain)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'algorithm': 'auto', 'n_neighbors': 3, 'weights': 'uniform'}\n",
      "Best Accuracy: 0.8017006802721088\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn KNN Predictions:\n",
      " [0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/sklearn/neighbors/_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "### Using the best Paramters \n",
    "knn_sklearn = KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm = 'auto')\n",
    "knn_sklearn.fit(xtrain_scaled, ytrain)\n",
    "\n",
    "# Make predictions\n",
    "predictions_sklearn = knn_sklearn.predict(xtest_scaled)\n",
    "print(\"Scikit-learn KNN Predictions:\\n\", predictions_sklearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AccuracyL 0.84\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.83        29\n",
      "           1       0.87      0.81      0.84        32\n",
      "\n",
      "    accuracy                           0.84        61\n",
      "   macro avg       0.84      0.84      0.84        61\n",
      "weighted avg       0.84      0.84      0.84        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(ytest, predictions_sklearn)\n",
    "print(f\"AccuracyL {accuracy:.2f}\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print precision, recall, and F1 score\n",
    "print(\"Classification Report:\\n\", classification_report(ytest, predictions_sklearn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing algorithm but before that trying to find out the best algorithm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV # Performing the GridSearchCV to find the best hyperparameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Prepare configuration for cross-validation test harness\n",
    "seed = 7\n",
    "\n",
    "# Prepare models and their hyperparameter grids\n",
    "models = []\n",
    "param_grids = {\n",
    "    'LR': {'C': [0.01, 0.1, 1, 10, 100]},\n",
    "    'LDA': {},\n",
    "    'KNN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},\n",
    "    'DTC': {'max_depth': [None, 5, 10, 15]},\n",
    "    'NB': {},\n",
    "    'SVM': {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']},\n",
    "    'RF': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'GBC': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1, 1]},\n",
    "    'SGD': {'loss': ['hinge', 'log', 'squared_loss'], 'penalty': ['l2', 'l1']},\n",
    "    'LGBM': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n",
    "    'XGB': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "}\n",
    "\n",
    "# Initialize list for best models and results\n",
    "best_models = []\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "# Evaluate each model with GridSearchCV\n",
    "for name, model in models:\n",
    "    start_time = time.time()\n",
    "    param_grid = param_grids.get(name, {})\n",
    "    \n",
    "    # If there's a parameter grid for the model, perform GridSearchCV\n",
    "    if param_grid:\n",
    "        grid = GridSearchCV(model, param_grid, cv=5, scoring=scoring, n_jobs=-1)\n",
    "        grid.fit(X_train_scaled, y_train_values)\n",
    "        best_model = grid.best_estimator_\n",
    "        best_score = grid.best_score_\n",
    "        best_models.append(best_model)\n",
    "        msg = \"%s: Best Score: %f %s seconds\" % (name, best_score, time.time() - start_time)\n",
    "    else:\n",
    "        # If no parameters to tune, just fit the model with default parameters\n",
    "        cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "        cv_results = cross_val_score(model, X_train_scaled, y_train_values, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f) %s seconds\" % (name, cv_results.mean(), cv_results.std(), time.time() - start_time)\n",
    "    \n",
    "    print(msg)\n",
    "\n",
    "# boxplot algorithm comparison for best models\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison with Best Parameters')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
